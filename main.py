#!/usr/bin/env python3
# -*- coding: utf-8 -*-



import os
import sys
import json
import time
import subprocess
import re
import tempfile
import threading
import random
from datetime import datetime
from pathlib import Path

# --- Setup Paths ---
# ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏° Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ffmpeg ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà Streamlit ‡∏°‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô
os.environ["PATH"] += os.pathsep + "/opt/homebrew/bin"
os.environ["PATH"] += os.pathsep + "/usr/local/bin"

# --- ‡∏™‡πà‡∏ß‡∏ô‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ (Setup) ---
# ‡πÇ‡∏´‡∏•‡∏î API Key ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå .env ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Key ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡∏´‡∏°‡∏∏‡∏ô‡πÄ‡∏ß‡∏µ‡∏¢‡∏ô
from pathlib import Path
env_file = Path(__file__).parent / '.env'
GEMINI_API_KEYS = []
if env_file.exists():
    with open(env_file) as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                os.environ[key] = value
                # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° API Key ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ GEMINI_API_KEY
                if key.startswith('GEMINI_API_KEY'):
                    GEMINI_API_KEYS.append(value)

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏µ‡∏¢‡πå‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î GEMINI_API_KEY
if GEMINI_API_KEYS and 'GEMINI_API_KEY' not in os.environ:
    os.environ['GEMINI_API_KEY'] = GEMINI_API_KEYS[0]

from concurrent.futures import ThreadPoolExecutor, as_completed

from utils import extract_meaningful_search_query, search_videos

from utils import extract_video_id, format_transcript, get_video_title, get_video_info, download_audio, extract_search_query_from_ai_result, extract_meaningful_search_query, format_time, parse_timestamp_to_seconds

def call_gemini_with_retry(prompt, audio_path=None, max_output_tokens=2048):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Gemini AI ‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏™‡∏≥‡∏£‡∏≠‡∏á (Retry & Fallback)
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏•‡∏±‡∏ö API Key ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ï‡πá‡∏° (Quota Full)
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏•‡∏∞ API Key
    """
    import google.generativeai as genai
    import time
    import os

    # ‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏£‡πá‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ‡πÑ‡∏ß
    models_to_try = [
        'gemini-1.5-pro-latest',
        'gemini-2.0-flash',
        'gemini-2.0-flash-exp',
        'gemini-flash-latest',
        'gemini-1.5-flash-8b-latest'
    ]

    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡∏Ñ‡∏µ‡∏¢‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö
    keys_to_try = GEMINI_API_KEYS.copy() if GEMINI_API_KEYS else [os.getenv('GEMINI_API_KEY')]
    
    # ‡∏™‡∏•‡∏±‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏µ‡∏¢‡πå‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏° ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏µ‡∏¢‡πå‡πÉ‡∏î‡∏Ñ‡∏µ‡∏¢‡πå‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
    import random
    random.shuffle(keys_to_try)
    
    last_error = None
    for model_name in models_to_try: # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ï‡∏≤‡∏°‡∏£‡∏∏‡πà‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å Flash ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
        for api_key in keys_to_try: # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ï‡∏≤‡∏° API Key ‡∏ó‡∏µ‡πà‡∏°‡∏µ
            try:
                genai.configure(api_key=api_key)
                model = genai.GenerativeModel(model_name)
                
                content_payload = [prompt]
                if audio_path:
                    ext = os.path.splitext(audio_path)[1].lower()
                    mime_type = 'audio/mp3' if ext == '.mp3' else f'audio/{ext[1:]}' if ext else 'audio/mp4'
                    audio_file = genai.upload_file(audio_path, mime_type=mime_type)
                    
                    # ‡∏£‡∏≠‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå (‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏≠‡∏ä‡πà‡∏ß‡∏á‡∏™‡∏±‡πâ‡∏ô‡πÜ 1 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß)
                    file_status = genai.get_file(audio_file.name)
                    waited = 0
                    while file_status.state.name == "PROCESSING" and waited < 30:
                        time.sleep(1); waited += 1
                        file_status = genai.get_file(audio_file.name)
                    
                    if file_status.state.name == "FAILED": continue
                    content_payload.append(audio_file)
                
                response = model.generate_content(content_payload, 
                                                 generation_config={"max_output_tokens": max_output_tokens, "temperature": 0.0})
                
                # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏¥‡πâ‡∏á‡∏´‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à (Cleanup)
                if audio_path:
                    try: genai.delete_file(audio_file.name)
                    except: pass
                
                return response.text.strip()
            except Exception as e:
                last_error = e
                if "429" in str(e): continue
                if "404" in str(e).lower(): break # Try next model
                continue
    
    return {"error": f"API_QUOTA_EXCEEDED: {last_error}"}

def summarize_with_gemini(transcript, title="", max_retries=2):
    """
    ‡πÉ‡∏ä‡πâ Google Gemini ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡πÅ‡∏•‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÅ‡∏•‡∏∞‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    """
    import google.generativeai as genai
    # Get API key from environment
    api_key = os.getenv('GEMINI_API_KEY')
    
    if not api_key:
        print("‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö GEMINI_API_KEY ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡∏™‡∏≥‡∏£‡∏≠‡∏á (Fallback)")
        return None
    
    prompt = f"""‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô "‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏¢‡πà‡∏≠" (Synopsis) ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î:
‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠: {title}
‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤:
{transcript}

‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á:
1. ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏¢‡πà‡∏≠ (Synopsis) ‡∏Ç‡∏≠‡∏á‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
2. ‡πÄ‡∏ô‡πâ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏•‡∏∞‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ß‡πà‡∏≤‡∏Ñ‡∏•‡∏¥‡∏õ‡∏ô‡∏µ‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏∞‡πÑ‡∏£ (‡πÑ‡∏°‡πà‡∏≠‡∏ô‡∏≤‡∏à‡∏≤‡∏£)
3. ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 3-5 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î (‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤)
4. ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"""
    
    return call_gemini_with_retry(prompt)

def extract_topics_with_gemini(transcript, title="", duration=None):
    """
    ‡πÉ‡∏ä‡πâ Google Gemini ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (Topics) ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
    """
    duration_str = f" [‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏Ñ‡∏∑‡∏≠ {duration}]" if duration else ""
    
    prompt = f"""‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ {title}{duration_str} ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á "‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç" (Topics) ‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à
‡∏ï‡∏•‡∏≠‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏•‡∏¥‡∏õ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤ ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÅ‡∏•‡∏∞‡πÑ‡∏î‡πâ‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡πÅ‡∏ï‡πà‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 5-10 ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠)
 
 ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å):
 1. ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö [HH:MM:SS] ‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏°‡∏≠
 2. **‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ (Topic Name)**: ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏£‡∏á ‡πÅ‡∏•‡∏∞‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡∏´‡πâ‡∏≤‡∏°‡∏Ñ‡∏≥‡πÄ‡∏ß‡∏¥‡πà‡∏ô‡πÄ‡∏ß‡πâ‡∏≠ ‡πÄ‡∏ä‡πà‡∏ô '‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á...', '‡πÇ‡∏à‡∏ó‡∏¢‡πå‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà...', '‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà...')
 3. **‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ (Description)**: ‡∏™‡∏£‡∏∏‡∏õ‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏û‡∏µ‡∏¢‡∏á 1 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏™‡∏≤‡∏£‡∏∞‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏ä‡πà‡∏ß‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
 4. **‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå**: ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏ô‡∏à‡∏ô‡∏à‡∏ö
 5. **‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤**: ‡∏´‡πâ‡∏≤‡∏°‡∏°‡πÇ‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ô‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å‡∏°‡∏≤‡πÉ‡∏™‡πà
 6. **‡∏†‡∏≤‡∏©‡∏≤**: ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
 
 ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£:
 - [00:01:23] ‡∏õ‡∏è‡∏¥‡∏Å‡∏¥‡∏£‡∏¥‡∏¢‡∏≤‡πÄ‡∏Ñ‡∏°‡∏µ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô: ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏Å‡∏£‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ö‡∏™‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
 - [00:05:45] ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÇ‡∏à‡∏ó‡∏¢‡πå‡πÄ‡∏•‡∏Ç‡∏¢‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á: ‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ê‡∏≤‡∏ô‡πÄ‡∏•‡∏Ç‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
 
 ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤:
 {transcript}"""
    
    topics_text = call_gemini_with_retry(prompt)
    if isinstance(topics_text, dict) and "error" in topics_text:
        return None
    
    topics = [t.strip().lstrip('-').strip() for t in topics_text.split('\n') if t.strip()]
    return topics

def count_unique_speakers(text):
    """
    ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô (‡πÄ‡∏ä‡πà‡∏ô '‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î 1', 'Speaker 2', ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô) ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    """
    if not text: return 0
    patterns = [r'(?:‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î|Speaker|‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡∏ó‡∏µ‡πà)\s*\d+', r'[^\[\]\n\r]+(?=:)']
    found_speakers = set()
    for line in text.split('\n'):
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ô‡∏û‡∏π‡∏î‡πÇ‡∏î‡∏¢‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤ (Timestamp) ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏≤ (Bold)
        # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á [00:00:00] ‡πÅ‡∏•‡∏∞ [00:00:00.500] ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤ [Start] TO [End]
        # ‡∏î‡∏±‡∏Å‡∏à‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô **...** ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏á ]
        match = re.search(r'\]\s*(?:[tT][oO]\s*\[[\d:\.]+\]\s*)?(?:\*\*)?\s*([^:*]+?)\s*(?:\*\*)?\s*:', line)
        if match:
            speaker = match.group(1).strip()
            if speaker and speaker not in ["SUMMARY", "TOPICS", "TRANSCRIPT", "‡∏ö‡∏ó‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≥‡∏ï‡πà‡∏≠‡∏Ñ‡∏≥"]:
                found_speakers.add(speaker)
    
    return len(found_speakers)

def merge_same_speaker_segments(text, force_merge=False):
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ö‡∏ó‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏ä‡πà‡∏ß‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
    force_merge: ‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô True ‡∏à‡∏∞‡∏£‡∏ß‡∏°‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡πÅ‡∏°‡πâ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏•‡∏≤‡∏á‡πÜ (‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢)
    """
    if not text:
        return text
        
    lines = text.split('\n')
    merged_lines = []
    
    current_speaker = None
    current_start = None
    current_end = None
    current_content = []
    
    # Pattern to match: [00:00:15] To [00:00:20] Speaker Name: Message
    # OR **[00:00:15] TO [00:00:20] ‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà 1** : Message
    # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö . ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏°‡∏¥‡∏•‡∏•‡∏¥‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ: [00:00:15.500]
    pattern = r'^(?:\*\*)?\[([\d:\.]+)\]\s*[tT][oO]\s*\[([\d:\.]+)\]\s*(.+?)(?:\*\*)?\s*:\s*(.*)'
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        match = re.match(pattern, line)
        if match:
            start_ts, end_ts, speaker, content = match.groups()
            speaker = speaker.strip().replace('**', '') # Clean up any stray bold markers
            
            # ‡∏´‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡∏≤‡∏á‡πÜ (‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î, Speaker, ‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà) ‡∏´‡πâ‡∏≤‡∏°‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ Merge 
            # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Gemini ‡πÄ‡∏´‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏ô‡∏û‡∏π‡∏î ‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡πÅ‡∏¢‡∏∞‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏î‡πâ
            # ‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ force_merge=True (‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢)
            is_generic = (speaker in ["‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î", "Speaker"] or speaker.startswith("‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà") or speaker.startswith("‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà")) and not force_merge
            
            if speaker == current_speaker and not is_generic:
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏ß‡∏•‡∏≤ (Gap detection): ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏´‡∏¢‡∏∏‡∏î‡∏û‡∏π‡∏î‡∏ô‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô 3 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‡πÉ‡∏´‡πâ‡πÅ‡∏¢‡∏Å‡πÅ‡∏ó‡πá‡∏Å‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏´‡∏°‡πà
                try:
                    prev_end_sec = parse_timestamp_to_seconds(current_end)
                    new_start_sec = parse_timestamp_to_seconds(start_ts)
                    if new_start_sec and prev_end_sec and (new_start_sec - prev_end_sec) > 3.0:
                        if current_speaker:
                            merged_lines.append(f"**[{current_start}] TO [{current_end}] {current_speaker}** : {' '.join(current_content)}")
                        
                        current_speaker = speaker
                        current_start = start_ts
                        current_end = end_ts
                        current_content = [content.strip()]
                        continue
                except: pass

                # ‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡πÄ‡∏î‡∏¥‡∏° ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
                current_end = end_ts
                current_content.append(content.strip())
            else:
                # ‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡πÉ‡∏´‡∏°‡πà ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                if current_speaker:
                    merged_lines.append(f"**[{current_start}] TO [{current_end}] {current_speaker}** : {' '.join(current_content)}")
                
                # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ä‡πà‡∏ß‡∏á‡πÉ‡∏´‡∏°‡πà
                current_speaker = speaker
                current_start = start_ts
                current_end = end_ts
                current_content = [content.strip()]
        else:
            # ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö (‡πÄ‡∏ä‡πà‡∏ô ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á, ‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ, ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏¢‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î)
            # ‡∏´‡∏≤‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡∏≠‡∏¢‡∏π‡πà ‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            if current_speaker:
                merged_lines.append(f"**[{current_start}] TO [{current_end}] {current_speaker}** : {' '.join(current_content)}")
                current_speaker = None
            
            # ‡∏Ñ‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ß‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
            merged_lines.append(line)
            
    # ‡∏≠‡∏¢‡πà‡∏≤‡∏•‡∏∑‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    if current_speaker:
        merged_lines.append(f"**[{current_start}] TO [{current_end}] {current_speaker}** : {' '.join(current_content)}")
        
    return '\n\n'.join(merged_lines)

def clean_invalid_timestamps(text, duration_seconds):
    """
    ‡∏•‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤ (Timestamp) ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠
    ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏•‡∏ö "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏≠‡∏ô" ‡∏Ç‡∏≠‡∏á AI (‡πÄ‡∏ä‡πà‡∏ô ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏ã‡∏≤‡∏Å)
    """
    if not text or duration_seconds <= 0: return text
    
    from utils import parse_timestamp_to_seconds
    
    # 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ (Topics): "- [00:05:30] ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠..."
    # 2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ö‡∏ó‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢ (Transcript): "[00:05:30] ‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î..."
    
    lines = text.split('\n')
    cleaned_lines = []
    
    # ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
    last_content = None
    repetition_count = 0
    
    for line in lines:
        # ‡∏Ç‡πâ‡∏≤‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏á
        if not line.strip():
            cleaned_lines.append(line)
            continue
            
        # ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏≤‡πÅ‡∏•‡∏∞‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á)
        # 1. ‡∏•‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏≤‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
        line_content = re.sub(r'(\*\*)?\s*\[\s*\d{1,2}:\d{2}(?::\d{2})?\s*\]\s*(\*\*)?', '', line)
        # 2. ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "To" ‡∏´‡∏£‡∏∑‡∏≠ "TO" 
        line_content = re.sub(r'\s+[Tt][Oo]\s+', ' ', line_content)
        # 3. ‡∏•‡∏ö‡∏õ‡πâ‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏≤)
        temp_content = re.sub(r'^(?:\*\*)?\s*(‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà|‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î|Speaker|‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà)\s*\d*\s*(?:\*\*)?\s*:', '', line_content.strip()).strip()
        current_content = temp_content
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡πÉ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (AI ‡∏´‡∏•‡∏≠‡∏ô)
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏ß‡∏•‡∏µ‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        words = line.split()
        if len(words) > 10:  # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß
            # Count consecutive repeated words/phrases
            max_repetition = 1
            current_repetition = 1
            for i in range(1, len(words)):
                if words[i] == words[i-1]:
                    current_repetition += 1
                    max_repetition = max(max_repetition, current_repetition)
                else:
                    current_repetition = 1
            
            # ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏≥‡πÄ‡∏î‡∏¥‡∏°‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô 5 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô ‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà AI ‡∏à‡∏∞‡∏´‡∏•‡∏≠‡∏ô
            if max_repetition > 5:
                print(f"   ‚ö†Ô∏è  ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á AI (‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ã‡πâ‡∏≥‡∏ã‡∏≤‡∏Å): {line[:100]}...")
                continue
            
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ï‡πâ‡∏≠‡∏á‡∏´‡πâ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏≠‡∏ô‡∏¢‡∏≠‡∏î‡∏ô‡∏¥‡∏¢‡∏° (Hallucinations)
        hallucination_patterns = [
            r"‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏ä‡∏°", r"‡∏≠‡∏¢‡πà‡∏≤‡∏•‡∏∑‡∏°‡∏Å‡∏î‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°", r"‡∏ù‡∏≤‡∏Å‡∏Å‡∏î‡πÑ‡∏•‡∏Ñ‡πå", r"‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°",
            r"Thanks for watching", r"Please subscribe", r"Stay tuned", r"Don't forget to like"
        ]
        is_hallucination = False
        for hp in hallucination_patterns:
            if re.search(hp, line, re.IGNORECASE) and len(line) < 100:
                is_hallucination = True
                break
        if is_hallucination:
            continue
            
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ß‡∏•‡∏≤ (Timestamps)
        ts_match = re.search(r'\[(\d{1,2}:\d{2}(?::\d{2})?)\]', line)
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏î‡∏ô‡∏ï‡∏£‡∏µ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏£‡∏ö‡∏Å‡∏ß‡∏ô (Noise tags)
        if re.match(r'^[\(\[\{](?:Music|Audio|Laughter|Silence|Applause|‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏î‡∏ô‡∏ï‡∏£‡∏µ|‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏£‡∏ö‡∏Å‡∏ß‡∏ô)[\)\]\}]$', line_content.strip(), re.IGNORECASE):
            continue
            
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏™‡∏≤‡∏£‡∏∞ (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠)
        if len(current_content) < 2 and not ts_match:
            continue

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î (‡πÄ‡∏ä‡πà‡∏ô ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°‡∏ã‡πâ‡∏≥‡∏ó‡∏∏‡∏Å‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
        
        if last_content and current_content == last_content and len(current_content) > 5:
            repetition_count += 1
            if repetition_count >= 2: # ‡∏•‡∏î‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 2 (‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô 3 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠)
                print(f"   ‚ö†Ô∏è  ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏≠‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î (‡∏ã‡πâ‡∏≥ {repetition_count} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á): {current_content[:50]}...")
                continue
        else:
            repetition_count = 0
            last_content = current_content
        
        if ts_match:
            ts_str = ts_match.group(1)
            ts_seconds = parse_timestamp_to_seconds(ts_str)
            if ts_seconds and ts_seconds > duration_seconds:
                # ‡∏´‡∏≤‡∏Å‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ ‡πÉ‡∏´‡πâ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏≠‡∏≠‡∏Å
                # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô "‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏≤‡∏¢" ‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
                h = int(duration_seconds // 3600)
                m = int((duration_seconds % 3600) // 60)
                sec = int(duration_seconds % 60)
                new_str = f"{h:02d}:{m:02d}:{sec:02d}"
                line = line.replace(f"[{ts_str}]", f"[{new_str}]")
                print(f"   ‚ö†Ô∏è  ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏≤‡∏Å [{ts_str}] ‡πÄ‡∏õ‡πá‡∏ô [{new_str}] (‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô {duration_seconds} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)")
            
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤ "To [HH:MM:SS]"
            to_match = re.search(r'To \[(\d{1,2}:\d{2}(?::\d{2})?)\]', line)
            if to_match:
                to_str = to_match.group(1)
                to_seconds = parse_timestamp_to_seconds(to_str)
                if to_seconds and to_seconds > duration_seconds:
                    # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤ "To"
                    h = int(duration_seconds // 3600)
                    m = int((duration_seconds % 3600) // 60)
                    sec = int(duration_seconds % 60)
                    new_to_str = f"{h:02d}:{m:02d}:{sec:02d}"
                    line = line.replace(f"To [{to_str}]", f"To [{new_to_str}]")
                    print(f"   ‚ö†Ô∏è  ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î 'To' ‡∏à‡∏≤‡∏Å [{to_str}] ‡πÄ‡∏õ‡πá‡∏ô [{new_to_str}]")
                    
        cleaned_lines.append(line)
        
    return '\n'.join(cleaned_lines)

def improve_readability_with_gemini(transcript, title="", duration=None):
    """
    ‡πÉ‡∏ä‡πâ Google Gemini ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ö‡∏ó‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
    """
    import google.generativeai as genai
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key: return transcript
    
    duration_str = f" [‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß: {duration}]" if duration else ""
    
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-flash-latest', generation_config={"temperature": 0.0})
        prompt = f"""‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ ‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (Format for Readability)
  
  ‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠: {title}{duration_str}
  
  **‡∏Å‡∏é‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô:**
  1. ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
  2. ‡∏£‡∏∞‡∏ö‡∏∏‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ) ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô [HH:MM:SS]
  3. ‡∏´‡πâ‡∏≤‡∏°‡∏™‡∏£‡∏∏‡∏õ ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏±‡∏î‡∏ó‡∏≠‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏≠‡∏≠‡∏Å
  4. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ú‡∏¥‡∏î ‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏ü‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÉ‡∏´‡πâ‡∏™‡∏•‡∏∞‡∏™‡∏•‡∏ß‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô (‡πÅ‡∏ï‡πà‡∏≠‡∏¢‡πà‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢)
  5. **‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô**
  
  Transcript:
  {transcript}"""
        
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        print(f"‚ö†Ô∏è  ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏á‡πà‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
        return transcript
# --- TRANSCRIPTION ENGINE: GOOGLE GEMINI NATIVE ---
# Legacy transcription logic removed for stability. 
# The system now uses Gemini-Native Audio processing for all transcription.



def process_audio_with_gemini(audio_path, transcript_hint="", title="", diarize=False, duration=None):
    """
    ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á Gemini ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏ó‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ
    transcript_hint: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏à‡∏≤‡∏Å YouTube ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏™‡∏∞‡∏Å‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤
    """
    import google.generativeai as genai
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Å‡πà‡∏≠‡∏ô (Gemini ‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î)
    file_size_mb = os.path.getsize(audio_path) / (1024*1024)
    MAX_FILE_SIZE_MB = 2000  # ‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏á Gemini ‡∏Ñ‡∏∑‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 2GB
    
    if file_size_mb > MAX_FILE_SIZE_MB:
        error_msg = f"‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ({file_size_mb:.1f} MB) - ‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏∑‡∏≠ {MAX_FILE_SIZE_MB} MB"
        print(f"‚ùå {error_msg}")
        raise Exception(f"FILE_TOO_LARGE: {error_msg}")
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î
    ext = os.path.splitext(audio_path)[1].lower()
    mime_type = 'audio/mp4' if not ext else f'audio/{ext[1:]}'
    if ext == '.mp3': mime_type = 'audio/mp3'
    
    # ‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô Prompt ‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÇ‡∏î‡∏¢‡∏•‡∏î‡∏†‡∏≤‡∏£‡∏∞‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î (Simplified for Speed)
    system_instruction = "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á (Audio Forensic) ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î (Diarization) ‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î"
    
    diarize_instruction = ""
    if diarize:
        diarize_instruction = """
‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏ô‡∏û‡∏π‡∏î (Active Ear Mode):
1. **‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏ó‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á**: ‡∏ü‡∏±‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á (Pitch), ‡∏ô‡πâ‡∏≥‡πÄ‡∏™‡∏µ‡∏¢‡∏á (Tone), ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î (Pacing) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏™‡∏µ‡πâ‡∏¢‡∏ß‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
2. **‡∏à‡∏±‡∏ö‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏∞‡∏™‡∏•‡∏±‡∏ö**: ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏ä‡πà‡∏ß‡∏á‡∏Å‡∏≤‡∏£‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö, ‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢, ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏ö‡∏á‡∏ö‡∏≠‡∏Å‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡∏ô
3. **‡∏´‡πâ‡∏≤‡∏°‡∏£‡∏ß‡∏ö‡∏Ñ‡∏ô (Strict Diarization)**: ‡∏´‡∏≤‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÅ‡∏°‡πâ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏á‡∏µ‡∏¢‡∏ö (Pause) ‡πÄ‡∏Å‡∏¥‡∏ô 1 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‡πÉ‡∏´‡πâ‡πÅ‡∏¢‡∏Å‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô '**‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà X**' ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
4. **‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏£‡∏¥‡∏á**: ‡∏´‡∏≤‡∏Å‡πÉ‡∏ô‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡πà‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏£‡∏¥‡∏á (‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏∏‡∏ì‡πÇ‡∏ï‡πâ‡∏á, ‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡πÄ‡∏≠‡∏Å) ‡πÅ‡∏ó‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏≤‡∏á‡∏ô‡∏¥‡∏ï‡∏¥‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á
5. **Silence Detection**: ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏û‡∏ö‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏á‡∏µ‡∏¢‡∏ö (Silence/Pause) ‡πÄ‡∏Å‡∏¥‡∏ô 2 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‡πÉ‡∏´‡πâ‡∏õ‡∏¥‡∏î‡∏ä‡πà‡∏ß‡∏á‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÅ‡∏•‡∏∞‡πÉ‡∏™‡πà‡∏™‡∏£‡∏∏‡∏õ‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ï‡∏≤‡∏°‡πÄ‡∏ó‡∏°‡πÄ‡∏û‡∏•‡∏ï‡πÄ‡∏õ‡πä‡∏∞‡πÜ"""

    prompt = f"""{system_instruction}
‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏ö‡∏°‡∏≤‡∏ô‡∏µ‡πâ ‡πÉ‡∏´‡πâ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡∏ñ‡∏≠‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:
{diarize_instruction}

[SUMMARY]
‡∏™‡∏£‡∏∏‡∏õ‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡πà‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß 3-5 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î (‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏™‡πà Timestamp)

[TOPICS]
‡∏î‡∏∂‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏û‡∏£‡πâ‡∏≠‡∏° Timestamp [HH:MM:SS.mmm] (‡∏Ñ‡∏±‡∏î‡πÄ‡∏≠‡∏≤‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à 5-8 ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤)
‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å:
1. ‡∏Ñ‡∏±‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÉ‡∏´‡∏ç‡πà‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡∏´‡πâ‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
2. ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ï‡πâ‡∏≠‡∏á "‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö" ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏Ç‡∏Ç‡πâ‡∏≠
3. ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: [‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô] ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠

[TRANSCRIPT] (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠: {duration if duration else "Unknown"})
‡∏ñ‡∏≠‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö """"""Frame-Perfect Structural Alignment & Segment Summarization""""""
‡∏Å‡∏é‡∏™‡∏≤‡∏Å‡∏•: ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏•‡∏±‡∏ö‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏•‡∏∞‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÜ ‡∏ï‡∏≤‡∏°‡πÄ‡∏ó‡∏°‡πÄ‡∏û‡∏•‡∏ï‡πÄ‡∏õ‡πä‡∏∞‡πÜ ‡πÇ‡∏î‡∏¢‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô "‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á" ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏•‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡πÄ‡∏Å‡∏¥‡∏ô 2 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
‡πÄ‡∏ó‡∏°‡πÄ‡∏û‡∏•‡∏ï‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏ï‡∏≤‡∏°‡πÄ‡∏õ‡πä‡∏∞‡πÜ (Example):
- **[00:00:00.000] TO [00:00:10.500] ‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà 1** : ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏ó‡πà‡∏≤‡∏ô ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤...
- **[00:00:10.500] TO [00:00:20.000] ‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà 2** : ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ú‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö
- `> [‡∏™‡∏£‡∏∏‡∏õ‡∏ä‡πà‡∏ß‡∏á‡∏ô‡∏µ‡πâ: ‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô (00:00:00 TO 00:00:20)]`
- **[00:00:20.000] TO [00:00:35.400] ‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà 1** : ‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏±‡∏ô‡∏Ñ‡∏∑‡∏≠...

‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å "Metronome-Level Sync" (‡∏Å‡∏≤‡∏£‡∏¢‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏´‡∏°‡πà):
1. **Waveform Metronome (Onset Detection)**: 
   - ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏î‡∏≤‡πÄ‡∏ß‡∏•‡∏≤! ‡πÉ‡∏´‡πâ‡∏î‡∏±‡∏Å‡∏ü‡∏±‡∏á "‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏±‡πà‡∏ô‡∏™‡∏∞‡πÄ‡∏ó‡∏∑‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á" (Waveform Energy) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
   - ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏û‡∏π‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏õ‡πä‡∏∞‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏°‡∏¥‡∏•‡∏•‡∏¥‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
2. **Clock Consistency**: 
   - ‡πÄ‡∏î‡∏¥‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Ç‡πá‡∏°‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á 1:1 ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏´‡∏£‡∏∑‡∏≠ Drift ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î
   - ‡∏ó‡∏∏‡∏Å‡πÄ‡∏™‡∏µ‡πâ‡∏¢‡∏ß‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡∏£‡∏∞‡∏ö‡∏∏‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ô‡∏û‡∏π‡∏î‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏á‡∏µ‡∏¢‡∏ö `>` ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
3. **Voice Fingerprinting & Naming**:
   - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏ó‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏ô‡∏û‡∏π‡∏î ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏´‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏¢‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏ô‡∏Ñ‡∏•‡∏¥‡∏õ
4. **Pattern & Continuity**: 
   - ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö: [HH:MM:SS.mmm] TO [HH:MM:SS.mmm] ‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î : ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
   - ‡∏ï‡∏•‡∏≠‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤: ‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (Zero-Gap)

‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠: {title}
‡∏Ñ‡∏≥‡πÉ‡∏ö‡πâ‡∏ö‡∏ó‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡πÄ‡∏î‡∏¥‡∏° (Transcript Hint - ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡πÄ‡∏ó‡∏°‡πÄ‡∏û‡∏•‡∏ï):
{transcript_hint}
"""

    # --- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ GEMINI (‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏¢‡πÉ‡∏ô) ---
    print(f"   ü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏∞‡∏ö‡∏ö‡∏´‡∏•‡∏≤‡∏¢ API Key...")
    ai_analysis_result = call_gemini_with_retry(prompt, audio_path=audio_path, max_output_tokens=65536)
    
    return ai_analysis_result


def process_text_with_gemini(transcript, title="", diarize=False, duration=None):
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ö‡∏ó‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ AI
    """
    diarize_instruction = ""
    if diarize:
        diarize_instruction = """
7. **‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î (Diarization) ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î**:
   - **‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ô‡πâ‡∏≥‡πÄ‡∏™‡∏µ‡∏¢‡∏á**: ‡πÅ‡∏¢‡∏Å‡πÅ‡∏¢‡∏∞‡∏Ñ‡∏ô‡∏û‡∏π‡∏î‡∏à‡∏≤‡∏Å‡πÇ‡∏ó‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á (Pitch/Tone) ‡πÅ‡∏•‡∏∞‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
   - **‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏ö‡∏£‡∏¥‡∏ö‡∏ó**: ‡∏Å‡∏≤‡∏£‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö, ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠, ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Å‡∏∞‡∏ó‡∏±‡∏ô‡∏´‡∏±‡∏ô
   - **‡∏´‡πâ‡∏≤‡∏°‡∏£‡∏ß‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß**: ‡∏´‡∏≤‡∏Å‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏°‡∏µ 2 ‡∏Ñ‡∏ô ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠ (‡πÄ‡∏ä‡πà‡∏ô **‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà 1**, **‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà 2**) ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
   - ‡∏´‡∏≤‡∏Å‡∏£‡∏π‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏™‡∏°‡∏≠ (‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏≠, ‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏ö‡∏µ)"""

    prompt = f"""‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å Transcript ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÅ‡∏ó‡πá‡∏Å‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î:

[SUMMARY]
‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 1 ‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤ (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 3-5 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î) ‡πÄ‡∏ô‡πâ‡∏ô‡∏™‡∏≤‡∏£‡∏∞‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö
**‡∏Å‡∏é‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**: ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏™‡πà Timestamp ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏î‡πÜ ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏ô‡∏µ‡πâ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î (No timestamps in summary)

[TOPICS]
‡∏î‡∏∂‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô [HH:MM:SS]
‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å:
1. ‡∏Ñ‡∏±‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÉ‡∏´‡∏ç‡πà‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 5-8 ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤)
2. ‡∏´‡πâ‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏¢‡∏¥‡∏ö‡∏¢‡πà‡∏≠‡∏¢
3. ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏±‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏°‡∏≠‡∏á‡∏†‡∏≤‡∏û‡∏≠‡∏≠‡∏Å‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
4. ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏™‡πà‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πâ‡∏≠
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: [00:05:30] ‡∏à‡∏¥‡∏ï‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô: ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå

[TRANSCRIPT] (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠: {duration if duration else "Unknown"})
**‡∏ö‡∏ó‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö "Tri-Level Precision Calibration"**
‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å (Authoritative Alignment):
1. **Scientific Accuracy**: ‡∏ñ‡∏≠‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ï‡∏≤‡∏°‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏Å‡∏î‡∏Ñ‡∏≥‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏ö‡∏ó (reduction of hallucinations)
2. **Voice Identity Matrix**: ‡πÅ‡∏¢‡∏Å‡πÅ‡∏¢‡∏∞‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡∏à‡∏≤‡∏Å‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î (Formal vs Informal) ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß
3. **Naming Hook**: ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÅ‡∏ó‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ó‡∏µ‡πà‡∏ó‡∏£‡∏≤‡∏ö
4. **Millisecond-Absolute Sync**: ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö [HH:MM:SS.mmm] TO [HH:MM:SS.mmm] ‡∏ï‡∏•‡∏≠‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏ö‡∏ó‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢ ‡∏´‡πâ‡∏≤‡∏°‡∏õ‡∏±‡∏î‡πÄ‡∏®‡∏©
5. **Continuous Loop & Metronome-Style Summarization**: ‡∏™‡∏•‡∏±‡∏ö‡∏™‡πÑ‡∏ï‡∏•‡πå "‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î" ‡πÅ‡∏•‡∏∞ "‡∏™‡∏£‡∏∏‡∏õ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (>)" ‡∏ï‡∏•‡∏≠‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ ‡∏´‡πâ‡∏≤‡∏°‡∏Ç‡∏≤‡∏î‡∏ä‡πà‡∏ß‡∏á ‡πÇ‡∏î‡∏¢‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà‡∏™‡∏£‡∏∏‡∏õ `> [‡∏™‡∏£‡∏∏‡∏õ‡∏ä‡πà‡∏ß‡∏á‡∏ô‡∏µ‡πâ: ...]` ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏´‡∏•‡∏±‡∏Å
{diarize_instruction}

‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏ä‡∏∑‡πà‡∏≠: {title}
Transcript ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö:
{transcript}

‡∏Å‡∏é‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°:
1. **‡∏†‡∏≤‡∏©‡∏≤**: ‡∏ï‡πâ‡∏≠‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏∞‡πÑ‡∏£ (‡πÑ‡∏ó‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©) ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•/‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡πâ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
2. **‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î**: ‡∏´‡∏≤‡∏Å Transcript ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á AI (‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏≥‡∏ã‡πâ‡∏≥‡πÜ ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏õ‡∏£‡∏∞‡∏´‡∏•‡∏≤‡∏î‡πÜ) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
3. **‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°**: ‡πÉ‡∏´‡πâ‡∏¢‡∏∂‡∏î‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πá‡∏à‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
4. **‡∏´‡πâ‡∏≤‡∏°‡∏´‡∏•‡∏≠‡∏ô**: ‡∏´‡πâ‡∏≤‡∏°‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏ã‡πâ‡∏≥‡πÜ ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å
"""
    analysis_text = call_gemini_with_retry(prompt, max_output_tokens=16384)
    return analysis_text


def generate_auto_summary(title, keywords, objective_sentences=None, transcript=""):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ Extractive Summarization
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ö‡∏ó‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏°‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
    ‡∏à‡∏∞‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏´‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
    """
    from pythainlp.tokenize import sent_tokenize
    from collections import defaultdict
    
    if not transcript or len(transcript) < 100:
        return f"‡∏Ñ‡∏•‡∏¥‡∏õ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö {', '.join([kw[0] for kw in keywords[:5]])}"
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    is_english = bool(re.search(r'[a-zA-Z]{3,}', transcript[:500]))  # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö 500 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏£‡∏Å
    thai_char_count = len(re.findall(r'[\u0E00-\u0E7F]', transcript[:500]))
    english_char_count = len(re.findall(r'[a-zA-Z]', transcript[:500]))
    
    # ‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 70% ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
    if english_char_count > thai_char_count and english_char_count > 50:
        is_english = True
    else:
        is_english = False
    
    # ‡πÅ‡∏¢‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ (Tokenize)
    if is_english:
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© ‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡∏à‡∏∏‡∏î, ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ï‡∏Å‡πÉ‡∏à, ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
        sentences = re.split(r'[.!?]+\s+', transcript)
    else:
        try:
            sentences = sent_tokenize(transcript)
        except:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏™‡∏≥‡∏£‡∏≠‡∏á: ‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
            sentences = re.split(r'[\.!?]|(?:‡∏Ñ‡∏£‡∏±‡∏ö|‡∏Ñ‡πà‡∏∞|‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö|‡∏ô‡∏∞‡∏Ñ‡∏∞|‡∏Ñ‡∏∞|‡∏ô‡∏∞)\s', transcript)
    
    # ‡∏•‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
    clean_sentences = []
    for sent in sentences:
        sent = sent.strip()
        # ‡∏•‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
        if len(sent) < 20:
            continue
        # ‡∏•‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏™‡∏£‡πâ‡∏≠‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≥
        if re.match(r'^(‡∏≠‡πà‡∏∞|‡πÄ‡∏≠‡πà‡∏≠|‡∏≠‡∏∑‡∏°|‡πÄ‡∏≠‡∏≠|‡∏ô‡∏∞|‡∏Ñ‡πà‡∏∞|‡∏Ñ‡∏£‡∏±‡∏ö|\s|>>)+$', sent):
            continue
        clean_sentences.append(sent)
    
    if not clean_sentences:
        return f"‡∏Ñ‡∏•‡∏¥‡∏õ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö {', '.join([kw[0] for kw in keywords[:5]])}"
    
    # ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ï‡∏≤‡∏°‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡∏ï‡πà‡∏≤‡∏á‡πÜ
    sentence_scores = defaultdict(float)
    keyword_list = [kw[0] for kw in keywords[:10]]  # ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç 10 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
    
    for i, sent in enumerate(clean_sentences):
        score = 0
        sent_lower = sent.lower()
        
        # 1. ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏ô‡∏≤‡πÅ‡∏ô‡πà‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡∏´‡∏•‡∏±‡∏Å)
        for keyword in keyword_list:
            if keyword in sent_lower:
                # ‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
                weight = (11 - keyword_list.index(keyword)) / 10
                score += weight * 2
        
        # 2. ‡πÇ‡∏ö‡∏ô‡∏±‡∏™‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á (‡∏ä‡πà‡∏ß‡∏á‡∏ï‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç)
        total_sentences = len(clean_sentences)
        if i < total_sentences * 0.15:  # 15% ‡πÅ‡∏£‡∏Å
            score += 1.5
        elif i > total_sentences * 0.85:  # 15% ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
            score += 1.0
        
        # 3. ‡πÇ‡∏ö‡∏ô‡∏±‡∏™‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß (‡πÄ‡∏ô‡πâ‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á)
        sent_length = len(sent)
        if 50 < sent_length < 200:
            score += 0.5
        elif 200 <= sent_length < 300:
            score += 0.3
        
        # 4. ‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ö‡∏≠‡∏Å‡πÄ‡∏•‡πà‡∏≤ (‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)
        if is_english:
            if re.search(r'\b(is|are|was|were|will|can|should|this|that|we|you|they)\b', sent_lower):
                score += 0.3
        else:
            if re.search(r'(‡∏Ñ‡∏∑‡∏≠|‡πÄ‡∏õ‡πá‡∏ô|‡∏ß‡πà‡∏≤|‡∏ó‡∏µ‡πà|‡∏ã‡∏∂‡πà‡∏á|‡πÇ‡∏î‡∏¢)', sent_lower):
                score += 0.3
        
        # 5. ‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠
        if is_english:
            if re.search(r'\b(today|this video|we will|going to|learn|teach|show|explain)\b', sent_lower):
                score += 1.0
        else:
            if re.search(r'(‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ|‡∏Ñ‡∏•‡∏¥‡∏õ‡∏ô‡∏µ‡πâ|‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á|‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö|‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á|‡πÅ‡∏ä‡∏£‡πå|‡∏™‡∏≠‡∏ô)', sent_lower):
                score += 1.0
        
        sentence_scores[i] = score
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
    num_sentences = min(8, max(3, len(clean_sentences) // 20))  # 3-8 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß
    top_indices = sorted(sentence_scores.keys(), key=lambda x: sentence_scores[x], reverse=True)[:num_sentences]
    
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á
    top_indices.sort()
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ
    summary_sentences = []
    for idx in top_indices:
        sent = clean_sentences[idx]
        # ‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÉ‡∏´‡πâ‡∏™‡∏∞‡∏≠‡∏≤‡∏î
        sent = re.sub(r'^(>>|\s|‡∏≠‡πà‡∏∞|‡πÄ‡∏≠‡πà‡∏≠)+', '', sent)
        sent = re.sub(r'(‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö|‡∏ô‡∏∞‡∏Ñ‡∏∞|‡∏Ñ‡∏£‡∏±‡∏ö|‡∏Ñ‡πà‡∏∞|‡∏Ñ‡∏∞|‡∏ô‡∏∞|‡∏≠‡πà‡∏∞|‡πÄ‡∏ô‡∏≠‡∏∞)+$', '', sent)
        sent = sent.strip()
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡∏°‡∏µ‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡πâ‡∏ß
        if len(sent) > 15:
            summary_sentences.append(sent)
    
    # ‡∏£‡∏ß‡∏°‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
    if not summary_sentences:
        return f"‡∏Ñ‡∏•‡∏¥‡∏õ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö {', '.join([kw[0] for kw in keywords[:5]])}"
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á (Flowing summary)
    summary = " ".join(summary_sentences)
    
    # ‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏´‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
    if is_english:
        try:
            from deep_translator import GoogleTranslator
            translator = GoogleTranslator(source='en', target='th')
            
            # ‡πÅ‡∏õ‡∏•‡∏ó‡∏µ‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô (Chunks) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß
            max_chunk_length = 4500
            if len(summary) > max_chunk_length:
                # ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡πÜ
                chunks = []
                current_chunk = ""
                for sent in summary_sentences:
                    if len(current_chunk) + len(sent) < max_chunk_length:
                        current_chunk += sent + " "
                    else:
                        chunks.append(current_chunk.strip())
                        current_chunk = sent + " "
                if current_chunk:
                    chunks.append(current_chunk.strip())
                
                # ‡πÅ‡∏õ‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô
                translated_chunks = []
                for chunk in chunks:
                    try:
                        translated = translator.translate(chunk)
                        translated_chunks.append(translated)
                    except:
                        translated_chunks.append(chunk)  # ‡πÉ‡∏ä‡πâ‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°‡∏´‡∏≤‡∏Å‡πÅ‡∏õ‡∏•‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
                
                summary = " ".join(translated_chunks)
            else:
                summary = translator.translate(summary)
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (Thai Prefix)
            if keywords:
                top_topics_en = ", ".join([kw[0] for kw in keywords[:5]])
                try:
                    top_topics_th = translator.translate(top_topics_en)
                    summary = f"‡∏Ñ‡∏•‡∏¥‡∏õ‡∏ô‡∏µ‡πâ‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á{top_topics_th} ‡πÇ‡∏î‡∏¢{summary}"
                except:
                    summary = f"‡∏Ñ‡∏•‡∏¥‡∏õ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö {top_topics_en} {summary}"
        except ImportError:
            # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ deep_translator ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
            if keywords:
                top_topics = ", ".join([kw[0] for kw in keywords[:5]])
                summary = f"‡∏Ñ‡∏•‡∏¥‡∏õ‡∏ô‡∏µ‡πâ‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á{top_topics} ‡πÇ‡∏î‡∏¢{summary}"
        except Exception as e:
            # ‡∏´‡∏≤‡∏Å‡πÅ‡∏õ‡∏•‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
            if keywords:
                top_topics = ", ".join([kw[0] for kw in keywords[:5]])
                summary = f"‡∏Ñ‡∏•‡∏¥‡∏õ‡∏ô‡∏µ‡πâ‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á{top_topics} ‡πÇ‡∏î‡∏¢{summary}"
    else:
        # ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏≤‡∏Å‡∏°‡∏µ
        if keywords:
            top_topics = ", ".join([kw[0] for kw in keywords[:5]])
            summary = f"‡∏Ñ‡∏•‡∏¥‡∏õ‡∏ô‡∏µ‡πâ‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á{top_topics} ‡πÇ‡∏î‡∏¢{summary}"
    
    # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß
    if len(summary) > 1000:
        summary = summary[:1000] + "..."
    
    return summary




def get_keywords(text, language="english", count=10):
    # ‡∏ï‡∏£‡∏£‡∏Å‡∏∞‡∏´‡∏•‡∏±‡∏Å: ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
    from collections import Counter
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    is_thai = bool(re.search(r'[\u0E00-\u0E7F]', text))
    
    if is_thai:
        try:
            from pythainlp.tokenize import word_tokenize
            from pythainlp.corpus import thai_stopwords
            
            # 1. ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥ (Tokenize)
            words = word_tokenize(text, engine="newmm")
            
            # 2. ‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡∏≥‡∏ü‡∏∏‡πà‡∏°‡πÄ‡∏ü‡∏∑‡∏≠‡∏¢ (Stop Words)
            stop_words = thai_stopwords()
            # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏≥‡∏ü‡∏∏‡πà‡∏°‡πÄ‡∏ü‡∏∑‡∏≠‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° (‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏≥‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢, ‡∏Ñ‡∏≥‡∏≠‡∏∏‡∏ó‡∏≤‡∏ô)
            custom_thai_stopwords = {
                "‡∏Ñ‡∏£‡∏±‡∏ö", "‡∏Ñ‡πà‡∏∞", "‡∏ô‡∏∞‡∏Ñ‡∏∞", "‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö", "‡∏ô‡∏∞", "‡∏à‡πâ‡∏∞", "‡∏à‡πâ‡∏≤", "‡πÄ‡∏ô‡∏≤‡∏∞",
                "‡πÄ‡∏≠‡πà‡∏≠", "‡∏≠‡πà‡∏≤", "‡πÅ‡∏ö‡∏ö", "‡∏Ñ‡∏∑‡∏≠", "‡∏Å‡πá", "‡πÅ‡∏•‡πâ‡∏ß", "‡πÄ‡∏•‡∏¢", "‡πÑ‡∏õ", "‡∏°‡∏≤",
                "‡∏ó‡∏µ‡πà", "‡∏ã‡∏∂‡πà‡∏á", "‡∏≠‡∏±‡∏ô", "‡∏ï‡∏±‡∏ß", "‡∏Ñ‡∏ô", "‡∏°‡∏±‡∏ô", "‡πÄ‡∏Ç‡∏≤", "‡πÄ‡∏ò‡∏≠", "‡πÄ‡∏£‡∏≤",
                "‡∏ô‡∏µ‡πâ", "‡∏ô‡∏±‡πâ‡∏ô", "‡πÇ‡∏ô‡πâ‡∏ô", "‡πÑ‡∏´‡∏ô", "‡∏≠‡∏∞‡πÑ‡∏£", "‡∏ó‡∏≥‡πÑ‡∏°", "‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£",
                "‡πÅ‡∏ï‡πà", "‡πÅ‡∏•‡∏∞", "‡∏´‡∏£‡∏∑‡∏≠", "‡∏ñ‡πâ‡∏≤", "‡∏´‡∏≤‡∏Å", "‡πÄ‡∏û‡∏£‡∏≤‡∏∞", "‡πÄ‡∏û‡∏∑‡πà‡∏≠", "‡πÇ‡∏î‡∏¢",
                "‡πÉ‡∏ô", "‡∏ö‡∏ô", "‡∏•‡πà‡∏≤‡∏á", "‡∏Ç‡πâ‡∏≤‡∏á", "‡∏´‡∏ô‡πâ‡∏≤", "‡∏´‡∏•‡∏±‡∏á", "‡∏à‡∏≤‡∏Å", "‡∏ñ‡∏∂‡∏á",
                "‡∏°‡∏µ", "‡πÄ‡∏õ‡πá‡∏ô", "‡∏≠‡∏¢‡∏π‡πà", "‡πÑ‡∏î‡πâ", "‡πÉ‡∏´‡πâ", "‡πÄ‡∏≠‡∏≤", "‡πÉ‡∏ä‡πâ", "‡∏ó‡∏≥",
                "‡∏à‡∏∞", "‡∏ï‡πâ‡∏≠‡∏á", "‡∏≠‡∏¢‡∏≤‡∏Å", "‡∏Ñ‡∏ß‡∏£", "‡∏≠‡∏≤‡∏à", "‡∏ö‡∏≤‡∏á", "‡∏ö‡πâ‡∏≤‡∏á", "‡πÜ",
                "‡∏´‡∏ô‡πà‡∏≠‡∏¢", "‡∏ô‡∏¥‡∏î", "‡∏ô‡πâ‡∏≠‡∏¢", "‡∏°‡∏≤‡∏Å", "‡∏à‡∏£‡∏¥‡∏á", "‡∏à‡∏±‡∏á", "‡∏™‡∏∏‡∏î",
                "‡∏î‡∏µ", "‡∏ä‡∏≠‡∏ö", "‡πÉ‡∏ä‡πà", "‡πÑ‡∏°‡πà", "‡πÄ‡∏õ‡∏•‡πà‡∏≤", "‡∏´‡∏£‡∏≠‡∏Å", "‡πÄ‡∏´‡∏£‡∏≠", "‡∏°‡∏±‡πâ‡∏¢",
                "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì", "‡∏Ç‡∏≠‡πÇ‡∏ó‡∏©", "‡∏û‡∏ö‡∏Å‡∏±‡∏ö", "‡∏≠‡∏µ‡∏Å‡πÅ‡∏•‡πâ‡∏ß", "‡∏ß‡∏±‡∏ô", "‡∏ô‡∏µ‡πâ",
                "‡∏¢‡πà‡∏≠‡∏•‡∏∞‡∏Ñ‡∏£", "‡∏ß‡∏±‡∏∞‡πÄ‡∏ß‡∏±‡∏Å", "‡∏°‡∏¢", "‡∏•‡∏≤‡∏•‡∏≤‡∏•‡∏≤", "‡∏•‡∏≤‡∏•‡∏≤" # ‡∏Ñ‡∏≥‡∏Ç‡∏¢‡∏∞‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö‡∏ã‡∏±‡∏ö‡πÑ‡∏ï‡πÄ‡∏ï‡∏¥‡πâ‡∏•
            }
            stop_words = stop_words.union(custom_thai_stopwords)
            
            # 3. ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥ (Filter)
            filtered_words = []
            for word in words:
                word = word.strip()
                if (word not in stop_words 
                    and len(word) > 1 
                    and not re.match(r'^\W+$', word) 
                    and not word.isnumeric()):
                    filtered_words.append(word)
            
            counter = Counter(filtered_words)
            return counter.most_common(count)
        except ImportError:
            print("‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô: ‡πÑ‡∏°‡πà‡∏û‡∏ö PyThaiNLP ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÅ‡∏ó‡∏ô")
    
    # ‡∏Å‡∏£‡∏ì‡∏µ‡∏™‡∏≥‡∏£‡∏≠‡∏á / ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
    words = re.findall(r'\w+', text.lower())
    stop_words = set(get_stop_words(language))
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏ü‡∏∏‡πà‡∏°‡πÄ‡∏ü‡∏∑‡∏≠‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
    stop_words.update(["is", "the", "that", "this", "it", "to", "of", "and", "a", "in", "you", "i", "we", "they", "he", "she", "so", "just", "like", "actually", "basically", "right", "okay", "yeah", "know", "mean", "think", "going", "want", "get", "got"])
    
    filtered_words = [word for word in words if word not in stop_words and len(word) > 2]
    counter = Counter(filtered_words)
    
    return counter.most_common(count)



def find_video_objective(text):
    """
    ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏∞‡πÄ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å "‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå" ‡∏´‡∏£‡∏∑‡∏≠ "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢" ‡∏Ç‡∏≠‡∏á‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠
    ‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡∏™‡πÅ‡∏Å‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏£‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á‡πÄ‡∏à‡∏ï‡∏ô‡∏≤
    """
    # ‡πÅ‡∏¢‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ (‡πÅ‡∏¢‡∏Å‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ Token ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)
    # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ß‡πâ‡πÅ‡∏ö‡∏ö‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
    # ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏¢‡∏Å‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏¥‡∏ö
    # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏¢‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏£‡∏Å‡∏∞‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÉ‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô summarize_text ‡πÑ‡∏î‡πâ
    # ‡πÅ‡∏ï‡πà‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ regex ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß
    
    # ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    try:
        from pythainlp.tokenize import sent_tokenize
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏´‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ (‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏£‡∏Å‡∏∞‡πÄ‡∏î‡∏¥‡∏°)
        text = re.sub(r'(‡∏Ñ‡∏£‡∏±‡∏ö|‡∏Ñ‡πà‡∏∞|‡∏ô‡∏∞|‡πÄ‡∏ô‡∏≤‡∏∞|‡∏™‡∏¥|‡∏à‡πâ‡∏∞|‡∏à‡πâ‡∏≤)', r'\1\n', text)
        sentences = sent_tokenize(text[:len(text)//3], engine="whitespace+newline")
    except ImportError:
         sentences = re.split(r'[\.\n]', text[:len(text)//3])
    
    intent_patterns = [
        # Thai
        r"‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ.*?‡∏à‡∏∞‡∏û‡∏≤",
        r"‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ.*?‡∏à‡∏∞‡∏°‡∏≤",
        r"‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤",
        r"‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á",
        r"‡∏™‡∏≠‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á",
        r"‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö",
        r"‡∏°‡∏≤‡∏î‡∏π",
        r"‡∏û‡∏ö‡∏Å‡∏±‡∏ö",
        # English
        r"today.*?we.*?will",
        r"going.*?to.*?talk.*?about",
        r"learn.*?about",
        r"video.*?is.*?about",
        r"show.*?you.*?how",
        r"welcome.*?to"
    ]
    
    candidates = []
    for s in sentences:
        s = s.strip()
        if len(s) < 10: continue
        
        for pattern in intent_patterns:
            if re.search(pattern, s, re.IGNORECASE):
                candidates.append(s)
                break
    
    # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏ó‡∏ô‡∏≥‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (Introduction) ‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏à‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á‡∏´‡∏≤‡∏Å‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô
    return candidates[:2]


def process_video(url, diarize_mode=True):
    """
    ‡∏ï‡∏£‡∏£‡∏Å‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ (‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡∏à‡∏≤‡∏Å main() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ô‡∏≥‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ)
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    """
    print(f"\nüî• [DEBUG] ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô process_video ‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö URL: {url}")
    results = {
        'video_info': {},
        'full_text': "",
        'ai_analysis': "",
        'keywords': [],
        'selected_segments': [],
        'ai_topics': [],
        'ai_summary': "",
        'all_segments': [],
        'output_filename': "",
        'is_audio_processed': False,
        'platform': "",
        'video_title': "",
        'video_id': "",
        'speaker_count': 0,
        'error': None,
        'transcription_source': "Unknown"
    }

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á (Local File) ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    is_local_file = os.path.exists(url)
    
    # ‡∏´‡∏≤‡∏Å‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏≤‡∏ò‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô
    if not is_local_file and (url.startswith('/') or url.startswith('./') or url.startswith('~/')):
        directory = os.path.dirname(url)
        filename = os.path.basename(url)
        if os.path.exists(directory):
            try:
                all_files = os.listdir(directory)
                import difflib
                normalized_target = filename.lower().replace('!', '').replace('|', '').replace('  ', ' ')
                matches = []
                for f in all_files:
                    normalized_f = f.lower().replace('!', '').replace('|', '').replace('  ', ' ')
                    similarity = difflib.SequenceMatcher(None, normalized_target, normalized_f).ratio()
                    if similarity > 0.8:
                        matches.append((f, similarity))
                if matches:
                    matches.sort(key=lambda x: x[1], reverse=True)
                    url = os.path.join(directory, matches[0][0])
                    is_local_file = True
            except: pass

    if is_local_file:
        video_title = os.path.basename(url)
        video_id = os.path.splitext(video_title)[0]
        platform = "Local File"
    else:
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ID ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î UnboundLocalError
        import hashlib
        video_id = hashlib.md5(url.encode()).hexdigest()[:10]
        
        print(f"üé¨ Starting process for URL: {url}")
        video_info = get_video_info(url)
        if not video_info:
            print("‚ö†Ô∏è Metadata extraction failed/blocked, using fallback info.")
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Metadata ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏Å‡∏≤‡∏£‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏≠‡∏≤‡∏à‡∏¢‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏õ‡∏Å‡∏ï‡∏¥
            video_title = f"Video_{video_id}" 
            platform = "Web (Fallback)"
            video_info = {'title': video_title, 'id': video_id, 'platform': platform}
        else:
            video_title = video_info.get('title', 'Unknown Title')
            video_id = video_info.get('id', video_id) # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠
            platform = video_info.get('platform', 'unknown')
        
        results['video_info'] = video_info

    results['video_title'] = video_title
    results['video_id'] = video_id
    results['platform'] = platform

    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ (Duration)
    duration_seconds = 0
    if not is_local_file and 'video_info' in results and results['video_info'].get('duration'):
        duration_seconds = results['video_info']['duration']
    elif is_local_file:
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ pydub (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏´‡∏≤‡∏Å‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á ffmpeg)
            from pydub.utils import mediainfo
            info = mediainfo(url)
            duration_seconds = float(info.get('duration', 0))
        except:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏™‡∏≥‡∏£‡∏≠‡∏á: ‡πÉ‡∏ä‡πâ yt-dlp ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
            info = get_video_info(url)
            if info: duration_seconds = info.get('duration', 0)
    
    duration_fmt = "00:00:00"
    if duration_seconds > 0:
        h = int(duration_seconds // 3600)
        m = int((duration_seconds % 3600) // 60)
        sec = int(duration_seconds % 60)
        duration_fmt = f"{h:02d}:{m:02d}:{sec:02d}"
    
    results['duration_seconds'] = duration_seconds
    results['duration_fmt'] = duration_fmt

    cleaned_segments = []
    full_text = ""
    is_audio_processed = False
    ai_analysis_result = ""
    use_audio_fallback = False
    
    # --- STEP 1: Transcription (YouTube API or Audio Download) ---
    print(f"üì° [Step 1] Initializing Transcription for {platform}...")
    
    # Try YouTube API for transcripts (Manual or Auto-generated)
    if 'youtube' in platform.lower():
        try:
            from youtube_transcript_api import YouTubeTranscriptApi
            print(f"üé¨ Checking for YouTube transcripts (Manual or Auto-generated)...")
            transcript_list_obj = YouTubeTranscriptApi.list_transcripts(video_id)
            
            # 1. ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á Manual Transcript (‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î)
            # 2. ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ ‡πÅ‡∏•‡∏∞ diarize=True ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡πÉ‡∏ä‡πâ Audio Native ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥)
            target_transcript = None
            try:
                target_transcript = transcript_list_obj.find_manually_created_transcript(['th', 'en'])
                print(f"‚úÖ Found manual transcript.")
            except:
                if not diarize:
                    try:
                        target_transcript = transcript_list_obj.find_generated_transcript(['th', 'en'])
                        print(f"‚úÖ Found auto-generated transcript.")
                    except:
                        # ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏≠‡∏±‡∏ô‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏°‡∏µ
                        for t in transcript_list_obj:
                            target_transcript = t
                            print(f"‚úÖ Found transcript ({t.language}).")
                            break
                else:
                    print(f"‚ö†Ô∏è Diarization enabled: Bypassing auto-generated transcript for higher precision.")
            
            if target_transcript:
                transcript_data = target_transcript.fetch()
                from utils import format_transcript_with_timestamps, format_time
                timestamped_transcript = format_transcript_with_timestamps(transcript_data)
                for item in timestamped_transcript:
                    start_s = float(item['start'])
                    start_time_str = format_time(start_s)
                    full_text += f"[{start_time_str}] {item['text']}\n"
                full_text = full_text.strip()
                if len(full_text) > 50:
                    is_audio_processed = True
                    ai_analysis_result = full_text
                    results['transcription_source'] = f"YouTube {target_transcript.language} ({'Manual' if target_transcript.is_manually_created else 'Auto'})"
            else:
                print(f"‚ÑπÔ∏è No transcripts found on YouTube. Proceeding with audio processing.")
        except Exception as e:
            print(f"‚ö†Ô∏è YouTube API check failed: {e}")

    # If diarization is requested, we MUST run Gemini Audio even if we have YT transcript
    # (Because YT transcripts don't have speakers)
    should_run_audio = not is_audio_processed or diarize_mode
    
    if should_run_audio:
        # ‡∏´‡∏≤‡∏Å‡∏°‡∏µ Transcript ‡∏à‡∏≤‡∏Å YT ‡πÉ‡∏´‡πâ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏õ‡πá‡∏ô TO ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡πÉ‡∏ö‡πâ‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏´‡πâ Gemini
        transcript_hint = ""
        if is_audio_processed and full_text:
            hint_lines = []
            raw_lines = full_text.split('\n')
            for i in range(len(raw_lines)):
                l = raw_lines[i].strip()
                t_match = re.match(r'\[(\d{1,2}:\d{2}(?::\d{2})?(?:\.\d+)?)\]\s*(.*)', l)
                if t_match:
                    start_ts = t_match.group(1)
                    content = t_match.group(2)
                    # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏≤‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                    end_ts = start_ts # Default
                    if i + 1 < len(raw_lines):
                        next_match = re.match(r'\[(\d{1,2}:\d{2}(?::\d{2})?(?:\.\d+)?)\]', raw_lines[i+1].strip())
                        if next_match: end_ts = next_match.group(1)
                    hint_lines.append(f"[{start_ts}] TO [{end_ts}] : {content}")
            transcript_hint = '\n'.join(hint_lines)
        
        if platform == "Local File": audio_file = url
        else:
            temp_audio_path = os.path.join(tempfile.gettempdir(), f"audio_{video_id}")
            print(f"üì• Downloading audio from: {url[:80]}...")
            print(f"   üíæ Target path: {temp_audio_path}")
            audio_file = download_audio(url, temp_audio_path)
        
        if audio_file:
            print(f"‚úÖ Audio source ready: {audio_file}")
            try:
                # --- CORE TRANSCRIPTION: Gemini-Native Audio (Ultra-Precision Diarization) ---
                # ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö Pipeline ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (Audio -> Gemini Analysis)
                native_ai_result = process_audio_with_gemini(audio_file, transcript_hint, video_title, diarize=diarize_mode, duration=duration_fmt)
                
                if isinstance(native_ai_result, str):
                    # Find [TRANSCRIPT] case-insensitively
                    import re
                    parts = re.split(r'\[TRANSCRIPT\]|TRANSCRIPT:', native_ai_result, flags=re.IGNORECASE)
                    if len(parts) > 1:
                        print(f"‚úÖ Gemini-Native Audio success! Extracted transcript length: {len(native_ai_result)}")
                        ai_analysis_result = native_ai_result
                        is_audio_processed = True
                        results['transcription_source'] = "Gemini-Native Audio (High Precision)"
                        
                        transcript_part = parts[1]
                        # Clean up the transcript part from other tags
                        transcript_part = re.split(r'\[SUMMARY\]|SUMMARY:|\[TOPICS\]|TOPICS:', transcript_part, flags=re.IGNORECASE)[0]
                        full_text = transcript_part.strip()
                        print(f"‚úÖ Cleaned transcript length: {len(full_text)}")
                    else:
                        # If tag is missing but we have content, try to use it anyway
                        if len(native_ai_result.strip()) > 100:
                            print(f"‚ÑπÔ∏è Gemini-Native Audio result found without [TRANSCRIPT] tag, using as-is.")
                            ai_analysis_result = native_ai_result
                            full_text = native_ai_result.strip()
                            is_audio_processed = True
                            results['transcription_source'] = "Gemini-Native Audio (Raw)"
                        else:
                            print(f"‚ö†Ô∏è Gemini-Native Audio return invalid format or too short.")
                else:
                    print(f"‚ö†Ô∏è Gemini-Native Audio failed.")
                    if isinstance(native_ai_result, dict) and 'error' in native_ai_result:
                         results['error'] = f"Gemini Error: {native_ai_result['error']}"
            except Exception as e:
                print(f"‚ö†Ô∏è Audio process failed: {e}")
                results['error'] = f"Audio processing error: {str(e)}"
        elif platform != "Local File":
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå .error ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏£‡∏ì‡∏µ Web)
            temp_audio_path = os.path.join(tempfile.gettempdir(), f"audio_{video_id}")
            error_file = f"{temp_audio_path}.error"
            if os.path.exists(error_file):
                with open(error_file, "r", encoding="utf-8") as f:
                    download_error = f.read()
                results['error'] = f"Audio download failed: {download_error[:200]}"
                print(f"‚ö†Ô∏è Download error captured: {results['error']}")
                try: os.remove(error_file)
                except: pass
            else:
                results['error'] = "Audio download failed (unknown cause)"

    # FINAL FALLBACK: YouTube Auto-generated transcript
    if not is_audio_processed and 'youtube' in platform.lower():
        try:
            print(f"üé¨ Falling back to YouTube auto-generated transcript...")
            from youtube_transcript_api import YouTubeTranscriptApi
            transcript_list_obj = YouTubeTranscriptApi.list_transcripts(video_id)
            auto_transcripts = [t for t in transcript_list_obj if t.is_generated]
            if auto_transcripts:
                transcript_data = auto_transcripts[0].fetch()
                from utils import format_transcript_with_timestamps
                timestamped_transcript = format_transcript_with_timestamps(transcript_data)
                full_text = ""
                for item in timestamped_transcript:
                    start_s = int(item['start'])
                    start_time = f"{start_s // 3600:02d}:{(start_s % 3600) // 60:02d}:{start_s % 60:02d}"
                    full_text += f"[{start_time}] {item['text']}\n"
                full_text = full_text.strip()
                if len(full_text) > 50:
                    is_audio_processed = True
                    ai_analysis_result = full_text
                    results['transcription_source'] = "YouTube Auto Transcript (Fallback)"
                    print(f"‚úÖ Final fallback successful.")
        except: pass

    # ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ Transcript ‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á Error ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    if not is_audio_processed:
        error_detail = results.get('error') or "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á"
        
        print(f"\n‚ùå TRANSCRIPTION FAILED - Error Detail: {error_detail}\n")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Ñ‡∏™‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ Cookies ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        block_keywords = ['403', 'Forbidden', 'Sign in', 'confirm your age', 'bot', 'block']
        is_blocked = any(kw.lower() in str(error_detail).lower() for kw in block_keywords)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° error ‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á
        if is_blocked:
            error_msg = f"TRANSCRIPTION_FAILED: {error_detail}\n\nüí° **‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ**: ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏ö‡∏•‡πá‡∏≠‡∏Å ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏ä‡πâ **'Cookie Vault'** ‡∏ó‡∏µ‡πà‡πÅ‡∏ñ‡∏ö‡πÄ‡∏°‡∏ô‡∏π‡∏î‡πâ‡∏≤‡∏ô‡∏ã‡πâ‡∏≤‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ö‡∏•‡πá‡∏≠‡∏Å"
        elif "unavailable" in error_detail.lower() or "404" in error_detail:
            error_msg = f"TRANSCRIPTION_FAILED: {error_detail}\n\nüí° **‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ**: ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏≠‡∏≤‡∏à‡∏ñ‡∏π‡∏Å‡∏•‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏≠‡∏∑‡πà‡∏ô"
        elif "Diarization" in error_detail or "model" in error_detail.lower():
            error_msg = f"TRANSCRIPTION_FAILED: {error_detail}\n\nüí° **‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ**: ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ ‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏≠‡∏á‡∏£‡∏µ‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ó‡πÅ‡∏≠‡∏õ"
        else:
            error_msg = f"TRANSCRIPTION_FAILED: {error_detail}\n\nüí° **‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ**: ‡∏•‡∏≠‡∏á‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏≠‡∏∑‡πà‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï"

        if "tiktok" in url.lower() or (platform and "tiktok" in platform.lower()):
            return {'error': f'TIKTOK_FAILED: {error_detail}\n\nüí° **‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ**: TikTok ‡∏°‡∏±‡∏Å‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ Cookie Vault ‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ö‡∏î‡πâ‡∏≤‡∏ô‡∏ã‡πâ‡∏≤‡∏¢'}
            
        return {'error': error_msg}

    # --- Step 2: Gemini 1.5 Pro Analysis (Consolidated) ---
    if is_audio_processed:
        # ‡∏´‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß (‡∏à‡∏≤‡∏Å Gemini-Native Audio) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
        # ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ (‡πÄ‡∏ä‡πà‡∏ô ‡∏°‡∏≤‡∏à‡∏≤‡∏Å YouTube) ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡πÑ‡∏õ‡πÉ‡∏´‡πâ Gemini ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
        if ai_analysis_result and re.search(r'\[TRANSCRIPT\]|TRANSCRIPT:', ai_analysis_result, re.IGNORECASE):
            ai_combined_result = ai_analysis_result
            print(f"‚úÖ Using existing Gemini-Native analysis results.")
        elif full_text:
            print(f"ü§ñ [Step 2] Gemini 1.5 Pro deep analysis of transcript...")
            ai_combined_result = process_text_with_gemini(full_text, video_title, diarize=diarize_mode, duration=duration_fmt)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Gemini ‡∏™‡πà‡∏á Error ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if isinstance(ai_combined_result, dict) and 'error' in ai_combined_result:
                print(f"   ‚ö†Ô∏è Gemini Analysis Error: {ai_combined_result['error']}")
                ai_combined_result = None
        else:
            ai_combined_result = None

        results['ai_analysis'] = ai_combined_result if ai_combined_result else "AI Analysis failed to generate results."
        
        # Step 2.2: Parse results
        if ai_combined_result and isinstance(ai_combined_result, str):
            print(f"   üìù Parsing results ({len(ai_combined_result)} chars)...")
            # ‡πÉ‡∏ä‡πâ regex ‡∏ó‡∏µ‡πà‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á [TAG] ‡πÅ‡∏•‡∏∞ TAG:)
            # Regex ‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡πÑ‡∏õ‡πÇ‡∏î‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô Prompt
            # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ TAG ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö [TAG], TAG:, **TAG** ‡∏´‡∏£‡∏∑‡∏≠ **[TAG]**
            # SUMMARY capture
            sum_match = re.search(r'(?:\*\*|\[)?(?:SUMMARY|‡∏™‡∏£‡∏∏‡∏õ)(?:\]|\:|\*\*\]|\*\*\:)(.*?)(?=(?:\*\*|\[)?(?:TOPICS|TRANSCRIPT|‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠|‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤|‡∏ö‡∏ó‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢)(?:\]|\:|\*\*\]|\*\*\:)|$)', ai_combined_result, re.DOTALL | re.IGNORECASE)
            
            # TOPICS capture
            top_match = re.search(r'(?:\*\*|\[)?(?:TOPICS|‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠)(?:\]|\:|\*\*\]|\*\*\:)(.*?)(?=(?:\*\*|\[)?(?:SUMMARY|TRANSCRIPT|‡∏™‡∏£‡∏∏‡∏õ|‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤|‡∏ö‡∏ó‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢)(?:\]|\:|\*\*\]|\*\*\:)|$)', ai_combined_result, re.DOTALL | re.IGNORECASE)
            
            # TRANSCRIPT capture
            trans_match = re.search(r'(?:\*\*|\[)?(?:TRANSCRIPT|‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤|‡∏ö‡∏ó‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢)(?:\]|\:|\*\*\]|\*\*\:)(.*?)(?=(?:\*\*|\[)?(?:SUMMARY|TOPICS|‡∏™‡∏£‡∏∏‡∏õ|‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠)(?:\]|\:|\*\*\]|\*\*\:)|$)', ai_combined_result, re.DOTALL | re.IGNORECASE)
            
            if sum_match: 
                raw_summary = sum_match.group(1).strip()
                # Post-process: ‡∏•‡∏ö Timestamp ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
                clean_summary = re.sub(r'\[\d{1,2}:\d{2}(?::\d{2})?\]', '', raw_summary)
                results['ai_summary'] = clean_summary.strip()
            
            if top_match:
                raw_topics = top_match.group(1).strip()
                # ‡πÅ‡∏¢‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
                topic_list = [t.strip().lstrip('-*‚Ä¢ ').strip() for t in raw_topics.split('\n') if t.strip()]
                results['ai_topics'] = topic_list
            
            if trans_match:
                results['full_text'] = trans_match.group(1).strip()
                results['speaker_count'] = count_unique_speakers(results['full_text'])
            else:
                # Fallback for transcript if tag is missing: Preserve both timestamps and Summaries
                transcript_lines = [l for l in ai_combined_result.split('\n') if re.search(r'\[\d{1,2}:\d{2}(?::\d{2})?(?:\.\d+)?\]', l)]
                if transcript_lines:
                    results['full_text'] = '\n'.join(transcript_lines)
                    results['speaker_count'] = count_unique_speakers(results['full_text'])

        # Backup summary if AI failed
        if not results.get('ai_summary'):
            results['ai_summary'] = generate_auto_summary(video_title, keywords=[], transcript=full_text)
            
        # ‡∏£‡∏±‡∏ö‡∏ö‡∏ó‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢: ‡πÉ‡∏ä‡πâ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà Gemini ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç (Diarized) ‡∏ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡∏°‡∏≤ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö (Step 1)
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Gemini ‡∏™‡πà‡∏á‡∏ö‡∏ó‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß 50% ‡∏Ç‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö)
        gemini_transcript = results.get('full_text', "").strip()
        
        if gemini_transcript and (len(gemini_transcript) > len(full_text) * 0.5):
            final_transcript = gemini_transcript
            print(f"‚úÖ Using Diarized Gemini Transcript ({len(final_transcript)} chars)")
        else:
            final_transcript = full_text
            print(f"‚ö†Ô∏è Gemini Transcript seems incomplete or failed. Falling back to original Source.")
        
        # --- NEW: ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏ä‡πà‡∏ß‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ô‡∏Ñ‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (Force Merge Identified Speakers) ---
        if final_transcript:
            final_transcript = merge_same_speaker_segments(final_transcript, force_merge=True)
            
        results['full_text'] = final_transcript
        
        # ‡∏™‡∏Å‡∏±‡∏î‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (Keywords)
        results['keywords'] = get_keywords(final_transcript + " " + video_title)
        
        # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å‡∏ö‡∏ó‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
        results['speaker_count'] = count_unique_speakers(final_transcript)
        results['is_audio_processed'] = True
    else:
        return {'error': 'NO_TRANSCRIPT_OR_AUDIO: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏î‡πâ (‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á)'}

    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏Ç‡∏ô‡∏≤‡∏ô)
    if results.get('ai_topics'):
        print(f"   üé¨ Auto-searching related videos for {len(results['ai_topics'])} topics...")
        related_recommendations = {}
        
        def search_for_topic(topic_str):
            # ‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
            topic_clean = re.sub(r'\[\d{1,2}:\d{2}(?::\d{2})?\]', '', topic_str)
            topic_clean = re.sub(r'^[\-\*\d\.\s]+', '', topic_clean).strip()
            topic_title = topic_clean.split(":", 1)[0].strip() if ":" in topic_clean else topic_clean
            
            search_query = extract_meaningful_search_query(topic_title)
            if search_query:
                return topic_str, search_videos(search_query, max_results=3)
            return topic_str, []

        with ThreadPoolExecutor(max_workers=5) as executor:
            future_to_topic = {executor.submit(search_for_topic, t): t for t in results['ai_topics']}
            for future in as_completed(future_to_topic):
                t_str, vids = future.result()
                if vids:
                    related_recommendations[t_str] = vids
        
        results['related_recommendations'] = related_recommendations

    return results

def main():
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏ö‡πÅ‡∏°‡∏ô‡∏ô‡∏ß‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏±‡∏î‡πÅ‡∏¢‡πâ‡∏á‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö URL
    diarize_mode = True # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô True ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏∏
    args = sys.argv[1:]
    
    if '--quick' in args:
        diarize_mode = False
        args.remove('--quick')
    if '--diarize' in args:
        args.remove('--diarize')
    
    if not args:
         print("Usage: python main.py <video_url> [--quick]")
         sys.exit(1)
         
    url = args[0]
    print("\n=== Processing Video ===")
    
    results = process_video(url, diarize_mode=diarize_mode)
    
    if not results or results.get('error'):
        error_msg = results.get('error', 'Could not process video.') if results else 'Could not process video.'
        print(f"‚ùå Error: {error_msg}")
        sys.exit(1)
        
    video_title = results.get('video_title', 'Unknown Title')
    video_id = results.get('video_id', 'unknown')
    platform = results.get('platform', 'unknown')
    is_audio_processed = results.get('is_audio_processed', False)
    
    output_dir = tempfile.gettempdir()
    safe_title = re.sub(r'[\\/*?:"<>|]', '', video_title)
    safe_title = "".join(c for c in safe_title if c.isprintable())
    safe_title = re.sub(r'\s+', '_', safe_title.strip())[:100]
    output_filename = os.path.join(output_dir, f"{safe_title}.txt")
    
    print(f"\nüíæ Saving to: {output_filename}")
    
    with open(output_filename, "w", encoding="utf-8") as f:
        f.write(f"Video ID: {video_id}\n")
        f.write(f"Title: {video_title}\n")
        f.write(f"Platform: {platform}\n")
        f.write("=" * 80 + "\n\n")
        
        if is_audio_processed:
            f.write("=== AI Analysis (Summary & Transcript) ===\n\n")
            f.write(results['ai_analysis'])
        else:
            f.write("=== Full Transcript & Analysis ===\n\n")
            f.write(f"Summary: {results['ai_summary']}\n\n")
            if results['ai_topics']:
                f.write("Topics:\n")
                for t in results['ai_topics']: f.write(f"- {t}\n")
        
        # ‡∏•‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (Related Videos) ‡∏≠‡∏≠‡∏Å

    print(f"\n‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡πÑ‡∏ü‡∏•‡πå: {output_filename}")
    
    import subprocess
    import platform as plt
    try:
        if plt.system() == 'Darwin': subprocess.run(["open", "-a", "TextEdit", output_filename])
        elif plt.system() == 'Windows': os.startfile(output_filename)
        else: subprocess.run(["xdg-open", output_filename])
        print("üìÇ ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡πâ‡∏ß!")
    except: pass

if __name__ == "__main__":
    main()
